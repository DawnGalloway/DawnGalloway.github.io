[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "myblog",
    "section": "",
    "text": "senior project\n\n\nForecast\n\n\nArima\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsenior project\n\n\nForecast\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsenior project\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "DGalloway_Resume.html",
    "href": "DGalloway_Resume.html",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "Lijia Yu\n\n\n\n\n\n dawn.n.galloway@gmail.com\n github.com/DawnGalloway\n linkedin.com/in/dawnngalloway/\n +1 206-245-0710\n\n\n\n\n\nExperienced in statistical analysis, statistical learning models, and optimization methods.\nFull experience with next generation sequencing data analysis.\nHighly skilled in R, Bash, Perl, Python, LaTeX\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2022-11-28."
  },
  {
    "objectID": "DGalloway_Resume.html#title",
    "href": "DGalloway_Resume.html#title",
    "title": "Lijia Yu’s resume",
    "section": "Dawn Galloway",
    "text": "Dawn Galloway\n\nRBDC faculty liaison and sales."
  },
  {
    "objectID": "DGalloway_Resume.html#employment",
    "href": "DGalloway_Resume.html#employment",
    "title": "Lijia Yu’s resume",
    "section": "Employment",
    "text": "Employment\n\nDigital Engineering & Data Science Intern\nIdaho National Laboratory - Battelle Energy Alliance, LLC\nIdaho Falls, Idaho\nMarch 2022 - Present\n• Created forecast comparisons in R Markdown with the Forecast, Fable, and FKF packages in R which resulted in an order of magnitude improvement in accuracy • Made improvements to GUI using Vue2 and Vueplotly • Created, visualized, and presented Yolov5 experiment project plan\n\n\nData Science Project Lead\nRexburg Business Development Center for Stotz Equipment\nRexburg, Idaho\nJanuary 2022 - April 2022\n• Led team to create R package for client which pulls weather information from Mesonet and crop date from Snowflake to track and predict growing degree units • Troubleshoot project issues, research resources • Assign student consultants tasks based on experience and interest\n\n\nDepartment Office Assistant\nBrigham Young Univerity - Idaho\nRexburg, Id\nApril 2019 - October 2021\n• Received an award from the Department Chair for efficiency and contributions • Hired, trained, and led 3-5 student office employees per semester • Reduced time on calls by over 50% by redirecting commonly misdirected and generic calls, as well as, increasing visibility of commonly requested information"
  },
  {
    "objectID": "DGalloway_Resume.html#related-experience-and-volunteering",
    "href": "DGalloway_Resume.html#related-experience-and-volunteering",
    "title": "Lijia Yu’s resume",
    "section": "Related Experience and Volunteering",
    "text": "Related Experience and Volunteering\n\nData Science Society Boot Camp Instructor\nBrigham Young University - Idaho\nRexburg, Idaho\nApril 2022 - Present, September 2021 - December 2021\n\n\nData Science Consultant\nRexburg Development Business Center\nRexburg, Idaho\nSeptember 2021 - December 2021\n• Accessed weather API using the RNOA and RNPN packages to assist in predicting harvest dates based on crop and planting date • Discovered alternate data source with greater data breadth and predictive data • Presented findings to client\n\n\nData Science Consultant/Project Manager\nRexburg Business Development Center\nRexburg, Idaho\nApril 2021 - July 2021\n• Assigned tasks, led discussions, tracked project progress • Pulled machine data from John Deer API to help solve business problem\n\n\nData Science Society Officer\nBrigham Young University - Idaho\nApril 2021 - July 2021\n\n\nData Science Consultant\nRexburg Business Development Center\nRexburg, Idaho\nJanuary 2021 - April 2021\n• Contributed to Shiny dashboard for STEM occupation webpage for a local college • Created sample Shiny dashboard for Project Manager"
  },
  {
    "objectID": "DGalloway_Resume.html#education",
    "href": "DGalloway_Resume.html#education",
    "title": "Lijia Yu’s resume",
    "section": "Education",
    "text": "Education\n\nBrigham Young University - Idaho\nB.S. Data Science with Computer Science minor (in progress)\nExpected Graduation July 2024\n\n\nCertificates\nData Science Certificate, April 2021\nProgramming - Computer Science, December 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dawn Galloway",
    "section": "",
    "text": "LinkedIn"
  },
  {
    "objectID": "posts/first_post/index.html",
    "href": "posts/first_post/index.html",
    "title": "Senior Project",
    "section": "",
    "text": "For my senior project, I will be comparing forecasting methods and speeds in order to take advantage of recent performance improvements in the data warehouse at my place of employment. Though I won’t be able to share details, I will post tips and tricks for the packages I will be using and will share useful advice I learn about time series characteristics such as trends and seasonality, statistical concepts like autocorrelation and partial autocorrelation, and methods like ETS, TBATS, and ARIMA.\nSome of the packages I plan to use:\n\nForecast: A well known package for forecasting with time series. See: Automatic Time Series Forecasting: The forecast Package for R\nFable: Essentially version three of the Forecast package, but renamed as it now works with tsibbles. See: Fable\nFKF: Fast Kalman Filter. See: The Fast Kalman Filter\nKFAS: Exponential Family State Space Models in R.\nBenchmark: A package for performance comparisons. See: Microbenchmark"
  },
  {
    "objectID": "posts/forecast/index.html",
    "href": "posts/forecast/index.html",
    "title": "The Magic of Forecast",
    "section": "",
    "text": "The first thing you need to know about the Forecast package is that there is an incredibly helpful text by Rob J Hyndman and George Athanasopoulos titled Forecasting: Principles and Practice which explains relevant concepts and walks through almost every function with examples. If you would like to work through their examples, you can download the fpp2 package which includes Forecast and all of the data. I wish every package had such a helpful guide.\nSome other useful resources:\n\nThe Forecast package manual and vignette is available here\nHyndsight, Rob J. Hyndman’s blog\nCross Validated, Stack Exchange’s Q&A for Statistical Topics\n\nHyndman pretty actively answers questions both on his blog and on Cross Validated, so I suggest reading the comments if your question isn’t answered in a blog entry.\n\n\n\nWhen they refer to a time series, they are not referring to data with time information. They are referring to a specific data type, the time series. Unlike a data frame or tibble, there isn’t a column for times, rather a time series has a starting point, an ending point, and a frequency. Typically, a frequency of 1 is annual, 4 is quarterly, 12 is monthly, and so on.\n\n#install.packages(\"fpp2\")\nlibrary(fpp2)\nhead(austourists)\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n1999 30.05251 19.14850 25.31769 27.59144\n2000 32.07646 23.48796                  \n\nhead(goog200)\n\nTime Series:\nStart = 1 \nEnd = 6 \nFrequency = 1 \n[1] 392.8300 392.5121 397.3059 398.0113 400.4902 408.0957\n\n\nThe International Tourists to Australia data has quartly data with a start year of 1999, while the start for the Google daily closing stock price is arbitrary, representing an undated year.\nThe forecast package has some functions which are wrappers that, given data and a forecast horizon, call another function followed by a call to forecast(). For example, holt() calls ets() to fit the data and then forecast(). I found it useful for my purposes to call these functions separately as it allowed me greater control.\n\n\n\nForecast: Principles and Practices explains the forecasting methods and underlying math well, so I won’t duplicate their efforts, but I will share my notes in an attempt to save others time.\n\n\nThe findfrequency function can calculate the frequency in your data. This can be a useful check to ensure that your frequency assumption is correct before working with your data. I worked with data that we did not expect would have a frequency, but findfrequency() found a frequency of five. This caused me to test seasonal as well as non-seasonal forecast methods. The downside of this function is that in data with multiple frequencies, it will only return the most dominant one. In the case of the Australian tourists data, it returns a half-yearly frequency rather than quarterly.\n\nfindfrequency(austourists)\n\n[1] 2\n\n\nSimilarly, the functions ndiffs() and nsdiffs() will return the number of times the given data needs to be differenced in order to become stationary. The type of unit root test used can be set with the test argument set to “kpss”, “adf”, or “pp”. Remember that the null hypothesis for the KPSS test is the opposite of the hypothesis for the Augmented Dickey-Fuller and Phillips-Perron test.\n\ngoogd <- ndiffs(goog200)\npaste(googd ,\" difference is needed to make the goog200 data stationary.\")\n\naustd <- nsdiffs(austourists)\npaste(austd ,\" difference is needed to make the austourists data stationary.\")\n\n[1] \"1  difference is needed to make the goog200 data stationary.\"\n[1] \"1  difference is needed to make the austourists data stationary.\"\n\n\nForecast has functions like autoplot(), autolayer(), ggLagplot(), ggHistogram(), ggAcf(), and ggPacf(), which take package models and use ggplot to create appropriate plots. This simplifies the plotting process while allowing you to add to plots in the same way one would with any ggplot. Below I added a title, theme, and color to the line.\n\nautoplot(austourists) + \n  ggtitle(\"International Tourists Visits to Australia in Millions\") +\n  geom_line(color=austourists) +\n  theme_classic()\n\n\n\n\n Below I use the Holt method on the data twice, once with the damping argument set to TRUE with phi of 0.9 and once with it set to FALSE. This example is in the text, but with different data (Fpp2 7.2). When the damped argument is set to NULL, both options are tried and the best one is returned.\n\ntraining <- window(austourists, end=c(2010,4))\n\nfc <- holt(training, h=15)\nfc2 <- holt(training, damped=TRUE, phi = 0.9, h=15)\nautoplot(training) +\n  autolayer(fc, series=\"Holt's method\", PI=FALSE) +\n  autolayer(fc2, series=\"Damped Holt's method\", PI=FALSE) +\n  ggtitle(\"Forecasts from Holt's method\") + xlab(\"Year\") +\n  ylab(\"Air passengers in Australia (millions)\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme_classic()\ntail(austourists)\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n2014                   54.76076 59.83447\n2015 73.25703 47.69662 61.09777 66.05576\n\n\n\n\n\nWith the Forecast package, it’s easy to check residuals and accuracy.\n\ncheckresiduals(fc)\n\n\n    Ljung-Box test\n\ndata:  Residuals from Holt's method\nQ* = 88.192, df = 4, p-value < 2.2e-16\n\nModel df: 4.   Total lags used: 8\n\n(fc_acc <- accuracy(fc, austourists))\n\n                     ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set  0.9211998 6.571903 5.097607 -0.6464137 14.95336 1.778292\nTest set     -0.7941557 8.899599 7.198491 -4.4212146 14.65768 2.511182\n                   ACF1 Theil's U\nTraining set -0.2619964        NA\nTest set     -0.3637125 0.5573859\n\n\n\n\n\nValues in these objects can be easily accessed, most with base R’s $, though the accuracy values are stored as a matrix and must be referenced accordingly. For example, the test MAPE, 14.6576754, can be accessed with object_name[2,5].\n\n\n\n\nForecast imports several packages including stats, tseries, and urca. These also have some useful functions:\n\nFrom urca, the kpss test.\nFrom tseries, the adf.test\nFrom stats, the Box.test as well as start(), end(), time(), cycle(), and deltaat() which are helpful when working with time series because the times don’t exist as an accessible column in the data structure."
  },
  {
    "objectID": "posts/arima/index.html",
    "href": "posts/arima/index.html",
    "title": "Working with Forecast’s Arima",
    "section": "",
    "text": "ARIMA or AutoRegressive Integrated Moving Average models have three parts: autoregression AR(p), integration I(d), and moving average MA(q) and are often written ARIMA(p,d,q) (FPP2 8.1). Seasonal data may have seasonal autoregression, integration, and moving averages as well. The seasonal component is written with capital letters and followed by the number of yearly seasonal observations. ARIMA(p,d,q)(P,D,Q)[m] (FPP2 8.9).\n\n\nTo have Forecast calculate the appropriate ARIMA model for your data, simply pass the data to auto.arima.\n\nlibrary(fpp2)\n\nWarning: package 'fpp2' was built under R version 4.1.3\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n-- Attaching packages ---------------------------------------------- fpp2 2.4 --\n\n\nv ggplot2   3.3.5     v fma       2.4  \nv forecast  8.19      v expsmooth 2.3  \n\n\nWarning: package 'forecast' was built under R version 4.1.3\n\n\nWarning: package 'fma' was built under R version 4.1.3\n\n\nWarning: package 'expsmooth' was built under R version 4.1.3\n\n\n\n\n# create training set\ntraining <- window(austourists, end=c(2010,4))\n\n# plot the training set\nautoplot(training)\n\n\n\n# fit the model and forecast\n(tr_fit <- auto.arima(training))\n\nSeries: training \nARIMA(1,0,0)(1,1,0)[4] with drift \n\nCoefficients:\n         ar1     sar1   drift\n      0.4146  -0.4656  0.4612\ns.e.  0.1386   0.1323  0.1027\n\nsigma^2 = 5.707:  log likelihood = -99.77\nAIC=207.53   AICc=208.56   BIC=214.67\n\ntr_fc <- forecast(tr_fit, h=20)\n\nForecast quickly calculates an ARIMA(1,0,0)(1,1,0)[4] for the International Tourists to Australia data and forecasts 20 quarters ahead. However, you may not agree with the number of differences auto.arima uses to make the data stationary, so it’s a good idea to examine the ACF & PACF to verify auto.arima’s suggestion. If you disagree with auto.arima’s recommendation, you can use the d argument to force auto.arima to use the desired number of differences. You can also specify the order (non-seasonal components), seasonal (seasonal components), and include a mean or drift. Be aware that auto.arima will ignore a drift argument if the differencing is greater than 1.\n\n# test the forecast\ncheckresiduals(tr_fc)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0)(1,1,0)[4] with drift\nQ* = 3.1445, df = 6, p-value = 0.7905\n\nModel df: 2.   Total lags used: 8\n\n(tr_acc <- accuracy(tr_fc, austourists))\n\n                     ME    RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.03553001 2.20789 1.656523 -0.6591379 5.075788 0.5778755\nTest set     2.67808084 3.86986 2.964032  4.6861651 5.142123 1.0339977\n                    ACF1 Theil's U\nTraining set -0.04387455        NA\nTest set      0.39353623 0.2915726\n\n\nThe accuracy function can pull the test data from the full set and return multiple measures. \n\n\n\nIf auto.arima returns a model with residuals that indicate room for a better model or you prefer, the Arima function can be used. Note: Stats has the arima() function while Forecast has Arima(). The best references I have found for estimating an ARIMA model are:\n\nEsimation and Order Selection (FPP2 8.6)\nPenn State’s Identifying and Esimating ARIMA models from Stats 510 (Lesson 3) Their Stats 510 course is an excellent reference for understanding many forecasting methods.\n\n\n\nThe first step in estimating an arima model is to check to see whether the data has a trend or increasing level. In the plot above we see an upward trend. In the ACF below, the training data has large positive seasonal lags which slowly decrease toward zero. We can use ndiffs() or nsdiffs() to determine the number of first order differences needed to make the data stationary.\n\n# Check the ACF & PACF for stationarity\nggAcf(training)\nggPacf(training)\n\n# determine the number of seasonal and non-seasonal differences needed\nnsdiffs(training)\nsd_training <- diff(training, lag=4, differences=1)\nndiffs(sd_training)\n# Recheck the ACF & PACF\nggAcf(sd_training)\nggPacf(sd_training)\n\n\n\n[1] 1\n\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we recheck the ACF & PACF on the seasonally differenced training data (adove), we can see that the large seasonal lags have been removed and the data appears stationary.\n\n\n\nAnother feature of the tourist data is increasing quarterly variation, so we will transform the data using a Box-Cox transformation. More information on transformations is available in the Transformations and Adjustments chapter (FPP@ 3.2)\n\ntr_lam <- BoxCox.lambda(training)\nautoplot(training)\nautoplot(BoxCox(training,tr_lam))\n\n\n\n\n\n\n\n\n\n\n\nNow we use the ACF & PACF of the seasonally differenced data to estimate the terms for the ARIMA.\n\nggAcf(sd_training)\nggPacf(sd_training)\n\n\n\n\n\n\n\n\n\n\n\nFirst, we examine the seasonal lags (every fourth lag since the data is quarterly). In both the ACF & PACF the first quarterly lag is outside the line of significance. However, in the PACF the seasonal lags decay (taper off). This suggests a seasonal MA compenent of 1. Next, we examine the non-seasonal lags. In the ACF, the non-seasonal lags almost taper, but in the PACF, the non-seasonal lags almost cut off.\nThis suggests a non-seasonal ARIMA(1,0,0) and a seasonal ARIMA(0,1,1). Remember, the seasonal difference of one was previously determined.\n\n# fit then forecast\n(tr_ar <- Arima(training, order=c(1,0,0), seasonal=c(1,1,0), lambda = tr_lam))\n\nSeries: training \nARIMA(1,0,0)(1,1,0)[4] \nBox Cox transformation: lambda= -0.1541732 \n\nCoefficients:\n         ar1     sar1\n      0.7014  -0.5792\ns.e.  0.1146   0.1239\n\nsigma^2 = 0.002426:  log likelihood = 70.04\nAIC=-134.08   AICc=-133.48   BIC=-128.73\n\ntr_ar_fc <- forecast(tr_ar, h=20)\n\n# Check the forecast\ncheckresiduals(tr_ar_fc)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0)(1,1,0)[4]\nQ* = 9.8567, df = 6, p-value = 0.1308\n\nModel df: 2.   Total lags used: 8\n\n(tr_ar_acc <- accuracy(tr_ar_fc, austourists))\n\n                    ME     RMSE      MAE       MPE      MAPE      MASE\nTraining set 0.8973055 2.487752 1.914765  2.102611  5.701674 0.6679627\nTest set     7.7328096 9.181161 7.763529 13.999936 14.051119 2.7082947\n                   ACF1 Theil's U\nTraining set -0.2490813        NA\nTest set      0.6527384 0.6868026\n\n\n\n\n\nIn this example, manually selecting the values actually decreases the accuracy of the ARIMA, possibly because this method is less reliable when the first lags of both the ACF & PACF are positive (FPP2 8.5). However, with the data I was using, manually estimating the model did result in better accuracy measures."
  }
]