[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an aspiring data scientist, interested in using my skills to do research and development. I live in Rexburg, Idaho with my three children and expect to graduate in July of 2024."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Science",
    "section": "",
    "text": "Senior Project\n\n\nKalman Filters\n\n\nDLM\n\n\nKFAS\n\n\nFKF\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Project\n\n\nFable\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Project\n\n\nForecast\n\n\nArima\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Project\n\n\nForecast\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Project\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nDawn Galloway\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "DGalloway_Resume.html",
    "href": "DGalloway_Resume.html",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "Lijia Yu\n\n\n\n\n\n dawn.n.galloway@gmail.com\n github.com/DawnGalloway\n linkedin.com/in/dawnngalloway/\n +1 206-245-0710\n\n\n\n\n\nExperienced in statistical analysis, statistical learning models, and optimization methods.\nFull experience with next generation sequencing data analysis.\nHighly skilled in R, Bash, Perl, Python, LaTeX\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2022-12-15."
  },
  {
    "objectID": "DGalloway_Resume.html#title",
    "href": "DGalloway_Resume.html#title",
    "title": "Lijia Yu’s resume",
    "section": "Dawn Galloway",
    "text": "Dawn Galloway\n\nRBDC faculty liaison and sales."
  },
  {
    "objectID": "DGalloway_Resume.html#employment",
    "href": "DGalloway_Resume.html#employment",
    "title": "Lijia Yu’s resume",
    "section": "Employment",
    "text": "Employment\n\nDigital Engineering & Data Science Intern\nIdaho National Laboratory - Battelle Energy Alliance, LLC\nIdaho Falls, Idaho\nMarch 2022 - Present\n• Created forecast comparisons in R Markdown with the Forecast, Fable, and FKF packages in R which resulted in an order of magnitude improvement in accuracy • Made improvements to GUI using Vue2 and Vueplotly • Created, visualized, and presented Yolov5 experiment project plan\n\n\nData Science Project Lead\nRexburg Business Development Center for Stotz Equipment\nRexburg, Idaho\nJanuary 2022 - April 2022\n• Led team to create R package for client which pulls weather information from Mesonet and crop date from Snowflake to track and predict growing degree units • Troubleshoot project issues, research resources • Assign student consultants tasks based on experience and interest\n\n\nDepartment Office Assistant\nBrigham Young Univerity - Idaho\nRexburg, Id\nApril 2019 - October 2021\n• Received an award from the Department Chair for efficiency and contributions • Hired, trained, and led 3-5 student office employees per semester • Reduced time on calls by over 50% by redirecting commonly misdirected and generic calls, as well as, increasing visibility of commonly requested information"
  },
  {
    "objectID": "DGalloway_Resume.html#related-experience-and-volunteering",
    "href": "DGalloway_Resume.html#related-experience-and-volunteering",
    "title": "Lijia Yu’s resume",
    "section": "Related Experience and Volunteering",
    "text": "Related Experience and Volunteering\n\nData Science Society Boot Camp Instructor\nBrigham Young University - Idaho\nRexburg, Idaho\nApril 2022 - Present, September 2021 - December 2021\n\n\nData Science Consultant\nRexburg Development Business Center\nRexburg, Idaho\nSeptember 2021 - December 2021\n• Accessed weather API using the RNOA and RNPN packages to assist in predicting harvest dates based on crop and planting date • Discovered alternate data source with greater data breadth and predictive data • Presented findings to client\n\n\nData Science Consultant/Project Manager\nRexburg Business Development Center\nRexburg, Idaho\nApril 2021 - July 2021\n• Assigned tasks, led discussions, tracked project progress • Pulled machine data from John Deer API to help solve business problem\n\n\nData Science Society Officer\nBrigham Young University - Idaho\nApril 2021 - July 2021\n\n\nData Science Consultant\nRexburg Business Development Center\nRexburg, Idaho\nJanuary 2021 - April 2021\n• Contributed to Shiny dashboard for STEM occupation webpage for a local college • Created sample Shiny dashboard for Project Manager"
  },
  {
    "objectID": "DGalloway_Resume.html#education",
    "href": "DGalloway_Resume.html#education",
    "title": "Lijia Yu’s resume",
    "section": "Education",
    "text": "Education\n\nBrigham Young University - Idaho\nB.S. Data Science with Computer Science minor (in progress)\nExpected Graduation July 2024\n\n\nCertificates\nData Science Certificate, April 2021\nProgramming - Computer Science, December 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dawn Galloway",
    "section": "",
    "text": "LinkedIn"
  },
  {
    "objectID": "posts/first_post/index.html",
    "href": "posts/first_post/index.html",
    "title": "Senior Project",
    "section": "",
    "text": "For my senior project, I will be comparing forecasting methods and speeds in order to take advantage of recent performance improvements in the data warehouse at my place of employment. Though I won’t be able to share details, I will post tips and tricks for the packages I will be using and will share useful advice I learn about time series characteristics such as trends and seasonality, statistical concepts like autocorrelation and partial autocorrelation, and methods like ETS, TBATS, and ARIMA.\nSome of the packages I plan to use:\n\nForecast: A well known package for forecasting with time series. See: Automatic Time Series Forecasting: The forecast Package for R\nFable: Essentially version three of the Forecast package, but renamed as it now works with tsibbles. See: Fable\nFKF: Fast Kalman Filter. See: The Fast Kalman Filter\nKFAS: Exponential Family State Space Models in R.\nBenchmark: A package for performance comparisons. See: Microbenchmark"
  },
  {
    "objectID": "posts/forecast/index.html",
    "href": "posts/forecast/index.html",
    "title": "The Magic of Forecast",
    "section": "",
    "text": "The first thing you need to know about the Forecast package is that there is an incredibly helpful text by Rob J Hyndman and George Athanasopoulos titled Forecasting: Principles and Practice (2nd Ed) which explains relevant concepts and walks through almost every function with examples. If you would like to work through their examples, you can download the fpp2 package which includes Forecast and all of the data. I wish every package had such a helpful guide.\nSome other useful resources:\n\nForecast package manual\nForecast Vignette\nHyndsight, Rob J. Hyndman’s blog\nCross Validated, Stack Exchange’s Q&A for Statistical Topics\n\nHyndman pretty actively answers questions both on his blog and on Cross Validated, so I suggest reading the comments if your question isn’t answered in a blog entry.\n\n\n\nWhen they refer to a time series, they are not referring to data with time information. They are referring to a specific data type, the time series. Unlike a data frame or tibble, there isn’t a column for times, rather a time series has a starting point, an ending point, and a frequency. Typically, a frequency of 1 is annual, 4 is quarterly, 12 is monthly, and so on.\n\nlibrary(fpp2)\nhead(austourists)\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n1999 30.05251 19.14850 25.31769 27.59144\n2000 32.07646 23.48796                  \n\nhead(goog200)\n\nTime Series:\nStart = 1 \nEnd = 6 \nFrequency = 1 \n[1] 392.8300 392.5121 397.3059 398.0113 400.4902 408.0957\n\n\nThe International Tourists to Australia data has quartly data with a start year of 1999, while the start for the Google daily closing stock price is arbitrary, representing an undated year.\nThe forecast package has some functions which are wrappers that, given data and a forecast horizon, call another function followed by a call to forecast(). For example, holt() calls ets() to fit the data and then forecast(). I found it useful for my purposes to call these functions separately as it allowed me greater control.\n\n\n\nForecast: Principles and Practices explains the forecasting methods and underlying math well, so I won’t duplicate their efforts, but I will share my notes in an attempt to save others time.\n\n\nThe findfrequency function can calculate the frequency in your data. This can be a useful check to ensure that your frequency assumption is correct before working with your data. I worked with data that we did not expect would have a frequency, but findfrequency() found a frequency of five. This caused me to test seasonal as well as non-seasonal forecast methods. The downside of this function is that in data with multiple frequencies, it will only return the most dominant one. In the case of the Australian tourists data, it returns a half-yearly frequency rather than quarterly.\n\nfindfrequency(austourists)\n\n[1] 2\n\n\nSimilarly, the functions ndiffs() and nsdiffs() will return the number of times the given data needs to be differenced in order to become stationary. The type of unit root test used can be set with the test argument set to “kpss”, “adf”, or “pp”. Remember that the null hypothesis for the KPSS test is the opposite of the hypothesis for the Augmented Dickey-Fuller and Phillips-Perron test.\n\naustd <- nsdiffs(austourists)\npaste(austd ,\" difference is needed to make the austourists data stationary.\")\n\n[1] \"1  difference is needed to make the austourists data stationary.\"\n\n\nForecast has functions like autoplot(), autolayer(), ggLagplot(), ggHistogram(), ggAcf(), and ggPacf(), which take package models and use ggplot to create appropriate plots. This simplifies the plotting process while allowing you to add to plots in the same way one would with any ggplot. Below I added a title, theme, and color to the line.\n\nautoplot(austourists) + \n  ggtitle(\"International Tourists Visits to Australia in Millions\") +\n  geom_line(color=austourists) +\n  theme_classic()\n\n\n\n\n Below I use the Holt method on the data twice, once with the damping argument set to TRUE with phi of 0.9 and once with it set to FALSE. This example is in the text, but with different data (Fpp2 7.2). When the damped argument is set to NULL, both options are tried and the best one is returned.\n\ntraining <- window(austourists, end=c(2010,4))\n\nfc <- holt(training, h=15)\nfc2 <- holt(training, damped=TRUE, phi = 0.9, h=15)\nautoplot(training) +\n  autolayer(fc, series=\"Holt's method\", PI=FALSE) +\n  autolayer(fc2, series=\"Damped Holt's method\", PI=FALSE) +\n  ggtitle(\"Forecasts from Holt's method\") + xlab(\"Year\") +\n  ylab(\"Air passengers in Australia (millions)\") +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  theme_classic()\n\n\n\n\nThe checkresiduals function displays several plots and the results of the Ljung-Box test.\n\ncheckresiduals(fc)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Holt's method\nQ* = 88.192, df = 4, p-value < 2.2e-16\n\nModel df: 4.   Total lags used: 8\n\n\nThe accuracy method can take the entire data set and compare the portion set aside for testing to a forecast.\n\n(fc_acc <- accuracy(fc, austourists))\n\n                     ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set  0.9211998 6.571903 5.097607 -0.6464137 14.95336 1.778292\nTest set     -0.7941557 8.899599 7.198491 -4.4212146 14.65768 2.511182\n                   ACF1 Theil's U\nTraining set -0.2619964        NA\nTest set     -0.3637125 0.5573859\n\n\nValues in these objects can be easily accessed, most with base R’s $, though the accuracy values are stored as a matrix and must be referenced accordingly. For example, the test MAPE, 14.6576754, can be accessed with object_name[2,5].\n\n\n\n\nForecast imports several packages including stats, tseries, and urca. These also have some useful functions:\n\nFrom urca, the kpss test.\nFrom tseries, the adf.test\nFrom stats, the Box.test as well as start(), end(), time(), cycle(), and deltaat() which are helpful when working with time series because the times don’t exist as an accessible column in the data structure."
  },
  {
    "objectID": "posts/arima/index.html",
    "href": "posts/arima/index.html",
    "title": "Working with Forecast’s Arima",
    "section": "",
    "text": "ARIMA or AutoRegressive Integrated Moving Average models have three parts: autoregression AR(p), integration I(d), and moving average MA(q) and are often written ARIMA(p,d,q) (FPP2 8.1). Seasonal data may have seasonal autoregression, integration, and moving averages as well. The seasonal component is written with capital letters and followed by the number of yearly seasonal observations. ARIMA(p,d,q)(P,D,Q)[m] (FPP2 8.9).\n\n\nSubset the data to create a training set using window(). To have Forecast calculate the appropriate ARIMA model for your data, simply pass the data to auto.arima.\n\nlibrary(fpp2)\n\n# create training set\ntraining <- window(austourists, end=c(2010,4))\n\n# plot the training set\nautoplot(training)\n\n\n\n# fit the model and forecast\n(tr_fit <- auto.arima(training))\n\nSeries: training \nARIMA(1,0,0)(1,1,0)[4] with drift \n\nCoefficients:\n         ar1     sar1   drift\n      0.4146  -0.4656  0.4612\ns.e.  0.1386   0.1323  0.1027\n\nsigma^2 = 5.707:  log likelihood = -99.77\nAIC=207.53   AICc=208.56   BIC=214.67\n\ntr_fc <- forecast(tr_fit, h=20)\n\nForecast quickly calculates an ARIMA(1,0,0)(1,1,0)[4] for the International Tourists to Australia data and forecasts 20 quarters ahead. However, you may not agree with the number of differences auto.arima uses to make the data stationary, so it’s a good idea to examine the ACF & PACF to verify auto.arima’s suggestion. If you disagree with auto.arima’s recommendation, you can use the d argument to force auto.arima to use the desired number of differences. You can also specify the order (non-seasonal components), seasonal (seasonal components), and include a mean or drift. Be aware that auto.arima will ignore a drift argument if the differencing is greater than 1.\n\n# test the forecast\ncheckresiduals(tr_fc)\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0)(1,1,0)[4] with drift\nQ* = 3.1445, df = 6, p-value = 0.7905\n\nModel df: 2.   Total lags used: 8\n\n(tr_acc <- accuracy(tr_fc, austourists))\n\n                     ME    RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.03553001 2.20789 1.656523 -0.6591379 5.075788 0.5778755\nTest set     2.67808084 3.86986 2.964032  4.6861651 5.142123 1.0339977\n                    ACF1 Theil's U\nTraining set -0.04387455        NA\nTest set      0.39353623 0.2915726\n\n\nThe accuracy function can pull the test data from the full set and return multiple measures. \n\n\n\nIf auto.arima returns a model with residuals that indicate room for a better model or you prefer, the Arima function can be used. Note: Stats has the arima() function while Forecast has Arima(). The best references I have found for estimating an ARIMA model are:\n\nEsimation and Order Selection (FPP2 8.6)\nPenn State’s Identifying and Esimating ARIMA models from Stats 510 (Lesson 3) Their Stats 510 course is an excellent reference for understanding many forecasting methods.\n\n\n\nThe first step in estimating an arima model is to check to see whether the data has a trend or increasing level. In the plot above we see an upward trend. In the ACF below, the training data has large positive seasonal lags which slowly decrease toward zero. We can use ndiffs() or nsdiffs() to determine the number of first order differences needed to make the data stationary.\n\n# Check the ACF & PACF for stationarity\nggAcf(training)\nggPacf(training)\n\n# determine the number of seasonal and non-seasonal differences needed\nnsdiffs(training)\nsd_training <- diff(training, lag=4, differences=1)\nndiffs(sd_training)\n# Recheck the ACF & PACF\nggAcf(sd_training)\nggPacf(sd_training)\n\n\n\n[1] 1\n\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we recheck the ACF & PACF on the seasonally differenced training data (adove), we can see that the large seasonal lags have been removed and the data appears stationary.\n\n\n\nAnother feature of the tourist data is increasing quarterly variation, so we will transform the data using a Box-Cox transformation. More information on transformations is available in the Transformations and Adjustments chapter (FPP2 3.2)\n\ntr_lam <- BoxCox.lambda(training)\nautoplot(training)\nautoplot(BoxCox(training,tr_lam))\n\n\n\n\n\n\n\n\n\n\n\nNow we use the ACF & PACF of the seasonally differenced data to estimate the terms for the ARIMA.\n\nggAcf(sd_training)\nggPacf(sd_training)\n\n\n\n\n\n\n\n\n\n\n\nFirst, we examine the seasonal lags (every fourth lag since the data is quarterly). In both the ACF & PACF the first quarterly lag is outside the line of significance. However, in the PACF the seasonal lags decay (taper off). This suggests a seasonal MA compenent of 1. Next, we examine the non-seasonal lags. In the ACF, the non-seasonal lags almost taper, but in the PACF, the non-seasonal lags almost cut off.\nThis suggests a non-seasonal ARIMA(1,0,0) and a seasonal ARIMA(0,1,1). Remember, the seasonal difference of one was previously determined.\n\n# fit then forecast\n(tr_ar <- Arima(training, order=c(1,0,0), seasonal=c(1,1,0), lambda = tr_lam))\n\nSeries: training \nARIMA(1,0,0)(1,1,0)[4] \nBox Cox transformation: lambda= -0.1541732 \n\nCoefficients:\n         ar1     sar1\n      0.7014  -0.5792\ns.e.  0.1146   0.1239\n\nsigma^2 = 0.002426:  log likelihood = 70.04\nAIC=-134.08   AICc=-133.48   BIC=-128.73\n\ntr_ar_fc <- forecast(tr_ar, h=20)\n\n# Check the forecast\ncheckresiduals(tr_ar_fc)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,0)(1,1,0)[4]\nQ* = 9.8567, df = 6, p-value = 0.1308\n\nModel df: 2.   Total lags used: 8\n\n(tr_ar_acc <- accuracy(tr_ar_fc, austourists))\n\n                    ME     RMSE      MAE       MPE      MAPE      MASE\nTraining set 0.8973055 2.487752 1.914765  2.102611  5.701674 0.6679627\nTest set     7.7328096 9.181161 7.763529 13.999936 14.051119 2.7082947\n                   ACF1 Theil's U\nTraining set -0.2490813        NA\nTest set      0.6527384 0.6868026\n\n\n\n\n\nIn this example, manually selecting the values actually decreases the accuracy of the ARIMA, possibly because this method is less reliable when the first lags of both the ACF & PACF are positive (FPP2 8.5). However, with the data I was using, manually estimating the model did result in better accuracy measures."
  },
  {
    "objectID": "posts/fable/index.html",
    "href": "posts/fable/index.html",
    "title": "A Fable for the Future",
    "section": "",
    "text": "The Fable package is essentially version 3.0 of Forecast. However, the name change reflects some considerable updates. \nRather than using time series, Fable uses a data type called the tsibble, or time series tibble. In theory, this is similar to a tibble, a data type most people work with on a regular basis and should work similarly. This also means data can have a specific column for time stamps which can be helpful if the times are slightly irregular. In addition to changing the supported data type, Fable has also changed the way arguments are passed to functions in a way that is more consistent with models in the stats package. Most data scientists that use R have fit linear models using a y ~ x syntax, so this should feel familiar for those who start predicting with Fable. However, it may be confusing for those already familiar with Forecast.\nUseful references:\n\nForecasting: Principles and Practice (3rd Ed)\nFable\nFable Vignette\nFable Manual\nHyndsight, Rob J. Hyndman’s blog\nCross Validated, Stack Exchange’s Q&A for Statistical Topics\n\nIn reality, even though tsibbles work with dplyr, I found there was a significant learning curve to this data type even when familiar with tibbles. My intention was to convert the time series used for my last post on Forecast’s arima (International Tourists to Australia), to a tsibble and use it here. However, though the data displayed as a tsibble, the model function did not recognize it as a tsibble no matter how I tried. Instead, we will use the data and examples from the text FPP3 9.7 to illustrate some of the differences and similarities between Forecast and Fable.\n\n# install.packages('fpp3')\n#install.packages('fable')\nlibrary(fable)\nlibrary(tsibble)\nlibrary(tsbox)\nlibrary(fpp3)\nlibrary(dplyr)\n\nglobal_economy %>%\n  filter(Code == \"EGY\") %>%\n  autoplot(Exports) +\n  labs(y = \"% of GDP\", title = \"Egyptian exports\") +\n  theme_light()\n\n\n\n\nAbove is a plot of exports from Egypt, from 1960 to 2017, as a percentage of GDP. Notice, Fable does have an autoplot function.\n\n\n\nFable can estimate an ARIMA model much like auto.arima from the Forecast package, but it is combined with the ability to specify the ARIMA model into one function, ARIMA(). If data is passed in without specifying the model’s order, ARIMA() will select the best model.\n\nfit <- global_economy %>%\n  filter(Code == \"EGY\") %>%\n  model(ARIMA(Exports))\nreport(fit)\n\nSeries: Exports \nModel: ARIMA(2,0,1) w/ mean \n\nCoefficients:\n         ar1      ar2      ma1  constant\n      1.6764  -0.8034  -0.6896    2.5623\ns.e.  0.1111   0.0928   0.1492    0.1161\n\nsigma^2 estimated as 8.046:  log likelihood=-141.57\nAIC=293.13   AICc=294.29   BIC=303.43\n\n\n\n\n\nOne unique feature of Fable is that more than one model can be evaluated at a time. Fable will return a Mable or model table when more than one model is passed into the model function.\n\nfit <- global_economy %>%\n  filter(Code == \"EGY\") %>%\n  model(\n    Ets = ETS(Exports),\n    Auto = ARIMA(Exports),\n    Arima400 = ARIMA(Exports ~ pdq(4,0,0)))\nreport(fit)\n\n# A tibble: 3 x 12\n  Country      .model sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE   MAE ar_ro~1\n  <fct>        <chr>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <list> \n1 Egypt, Arab~ Ets     10.3    -184.  375.  375.  381.  9.95  24.1  2.45 <NULL> \n2 Egypt, Arab~ Auto     8.05   -142.  293.  294.  303. NA     NA   NA    <cpl>  \n3 Egypt, Arab~ Arima~   7.88   -141.  293.  295.  305. NA     NA   NA    <cpl>  \n# ... with 1 more variable: ma_roots <list>, and abbreviated variable name\n#   1: ar_roots\n\n\nMables make it easy to compare the performance of different models. Here the ARIMA model chosen by Fable has the best AICc.\n\n\n\nWe can then pass the mable to accuracy() and easily compare measures.\n\nfit %>%\n  accuracy() %>%\n  arrange(RMSE)\n\n# A tibble: 3 x 11\n  Country       .model .type      ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  <fct>         <chr>  <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 Egypt, Arab ~ Arima~ Trai~ -0.0219  2.68  2.11 -2.18  11.3 0.845 0.844 -0.0303\n2 Egypt, Arab ~ Auto   Trai~ -0.0465  2.74  2.17 -2.36  11.7 0.872 0.860 -0.0219\n3 Egypt, Arab ~ Ets    Trai~ -0.0669  3.15  2.45 -1.70  12.7 0.984 0.991  0.187 \n\n\nWhile the AICc favored Fable’s ARIMA selection, the RMSE is better for the ARIMA which we specified.\n\n\n\nFable allows us to pass a forecast horizon for each model using natural language.\n\nEgy_exp <- global_economy %>%\n  filter(Code == \"EGY\")\n\nfc <- fit %>%\n  forecast(h = \"5 years\")\nfc %>%\n  autoplot(Egy_exp)\n\n\n\n\n\n\n\nWhile much of the functionality is the same there are many things that are subtly different from Forecast. Some related to using tsibbles like needing to use TSibble’s difference() rather than Forecast’s diff() to make data stationary. Others are related to the package itself, like syntax and function name changes. For example, Forecast used summary() rather than report() to print an overview of a model or mable’s fit. While not significant, the number of subtle differences, can make it frustrating to work with Fable after becoming familiar with Forecast.\nA few important differences should be mentioned:  Both packages use state space equations on the back end. However, Forecast uses more C++ and is significantly faster at most tasks. However, if you want access to the hidden state’s values, you will need to use Fable."
  },
  {
    "objectID": "posts/kalman filters/index.html",
    "href": "posts/kalman filters/index.html",
    "title": "Abandon Hope All Ye Who Enter Here - Kalman Filters",
    "section": "",
    "text": "The title of this post probably seems overly dramatic, but I assure you, it isn’t. Understanding how Kalman filters work requires a decent understanding of linear algebra and using Kalman filter packages requires a crystal ball and an abundance of patience. For previous posts, I have included lists of helpful resources. Here I will include links with a quote from some of the posts, so you can comprehend what you are getting into.\nBilgin’s Kalman Filter for Dummies: “When I started doing my homework for Optimal Filtering for Signal Processing class, I said to myself :“How hard can it be?”. Soon I realized that it was a fatal mistake.\nThe whole thing was like a nightmare. Nothing made sense. The equations were composed of some ridiculously complex superscripted and subscripted variables combined with transposed matrices and untransposed some other stuff, which are totally unknowable to most of us.”  Smoothing a Time Series with a Kalman Filter in R: “Unfortunately, I have had a hard time understanding the literature I’ve been able to find on the Kalman filter. After I completed a graduate level class on Advanced Statistics, where we covered Least Squares in great depth, I thought that I’d have another run at the Kalman filter (I read somewhere that the Kalman filter evolved from Least Squares). Unfortunately, my new knowledge was still not that helpful in understanding the material on the Kalman filter.  One of the great things about R is that you don’t always have to understand how the R function is implemented. For example, the fact that a Least Squares function might be implemented with QR decomposition does concern the user of the R lm() function.   As it turns out, however, the R documentation for the Kalman filter is not terribly easy to understand either.”  Derive Yourself a Kalman Filter: “If you have already tried to understand Kalman filters, you may very likely have found yourself dumbfounded, as I was when I did.”  Len Kiefer’s Kalman Filter for a dynamic linear model in R: “Part of the fun of working with the Kalman Filter is that it’s ubiquitous and has spread across multiple disciplines so the notation is often slightly different.”  How a Kalman filter works, in pictures Kalman Filter Explained Simply The Mathematics of the Kalman Filter Kalman Filter Tutorial \nIn other words, Kalman Filters aren’t for the faint of heart. In addition to having complex formulas, each industry uses different notation; it can be hard to synthesize information from multiple sources.  While the links above may help you begin to grasp the conceps, they are little help in using R Kalman filter packages. Most packages are poorly written and not-supported. When I first started to experiment with Kalman Filters, I began with the DLM package because most sources said it had the best documentation. Unfortunately, best is a relative term. It was difficult to understand all of the variables and data types that needed to be passed into the functions. I took a break to improve my understanding of arima models, so I could create a filter for an arima. When I returned, the DLM package had been removed from Cran. Several other packages I had seen referenced were also no longer available, so I had to relegate myself to experimenting with FKF and KFAS. While both use kalman filters, the data types for the various arguments were different from each other and both required a decent understand of linear algebra to properly construct a model."
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "D Galloway Resume",
    "section": "",
    "text": "Lijia Yu\n\n\n\n\n\n dawn.n.galloway@gmail.com\n github.com/DawnGalloway\n linkedin.com/in/dawnngalloway/\n +1 206-245-0710\n\n\n\n\n\nExperienced in statistical analysis, statistical learning models, and optimization methods.\nFull experience with next generation sequencing data analysis.\nHighly skilled in R, Bash, Perl, Python, LaTeX\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2022-12-15."
  },
  {
    "objectID": "Resume.html#title",
    "href": "Resume.html#title",
    "title": "D Galloway Resume",
    "section": "Dawn Galloway",
    "text": "Dawn Galloway\n\nRBDC faculty liaison and sales."
  },
  {
    "objectID": "Resume.html#employment",
    "href": "Resume.html#employment",
    "title": "D Galloway Resume",
    "section": "Employment",
    "text": "Employment\n\nDigital Engineering & Data Science Intern\nIdaho National Laboratory - Battelle Energy Alliance, LLC\nIdaho Falls, Idaho\nMarch 2022 - Present\n• Created forecast comparisons in R Markdown with the Forecast, Fable, and FKF packages in R which resulted in an order of magnitude improvement in accuracy • Made improvements to GUI using Vue2 and Vueplotly • Created, visualized, and presented Yolov5 experiment project plan\n\n\nData Science Project Lead\nRexburg Business Development Center for Stotz Equipment\nRexburg, Idaho\nJanuary 2022 - April 2022\n• Led team to create R package for client which pulls weather information from Mesonet and crop date from Snowflake to track and predict growing degree units • Troubleshoot project issues, research resources • Assign student consultants tasks based on experience and interest\n\n\nDepartment Office Assistant\nBrigham Young Univerity - Idaho\nRexburg, Id\nApril 2019 - October 2021\n• Received an award from the Department Chair for efficiency and contributions • Hired, trained, and led 3-5 student office employees per semester • Reduced time on calls by over 50% by redirecting commonly misdirected and generic calls, as well as, increasing visibility of commonly requested information"
  },
  {
    "objectID": "Resume.html#related-experience-and-volunteering",
    "href": "Resume.html#related-experience-and-volunteering",
    "title": "D Galloway Resume",
    "section": "Related Experience and Volunteering",
    "text": "Related Experience and Volunteering\n\nData Science Society Boot Camp Instructor\nBrigham Young University - Idaho\nRexburg, Idaho\nApril 2022 - Present, September 2021 - December 2021\n\n\nData Science Consultant\nRexburg Development Business Center\nRexburg, Idaho\nSeptember 2021 - December 2021\n• Accessed weather API using the RNOA and RNPN packages to assist in predicting harvest dates based on crop and planting date • Discovered alternate data source with greater data breadth and predictive data • Presented findings to client\n\n\nData Science Consultant/Project Manager\nRexburg Business Development Center\nRexburg, Idaho\nApril 2021 - July 2021\n• Assigned tasks, led discussions, tracked project progress • Pulled machine data from John Deer API to help solve business problem\n\n\nData Science Society Officer\nBrigham Young University - Idaho\nApril 2021 - July 2021\n\n\nData Science Consultant\nRexburg Business Development Center\nRexburg, Idaho\nJanuary 2021 - April 2021\n• Contributed to Shiny dashboard for STEM occupation webpage for a local college • Created sample Shiny dashboard for Project Manager"
  },
  {
    "objectID": "Resume.html#education",
    "href": "Resume.html#education",
    "title": "D Galloway Resume",
    "section": "Education",
    "text": "Education\n\nBrigham Young University - Idaho\nB.S. Data Science with Computer Science minor (in progress)\nExpected Graduation July 2024\n\n\nCertificates\nData Science Certificate, April 2021\nProgramming - Computer Science, December 2018"
  }
]